{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ETTh1_without_missing.csv to c:\\Users\\Nassim\\Desktop\\M1\\Data_Science\\time_series_classification\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 753k/753k [00:00<00:00, 1.36MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "api.competition_download_file('time-series-classification-part-1','ETTh1_without_missing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'date': '2016-07-01', 'hour': 0, 'OT': 30.5310001373291}\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "def get_data_from_csv(path):\n",
    "    row = []\n",
    "    with open(path, 'r') as file:\n",
    "        i = 0\n",
    "        reader = csv.reader(file, delimiter=';')\n",
    "        data = {}\n",
    "        for row in reader:\n",
    "            if(i != 0):\n",
    "                row_values = {}\n",
    "                row_values[\"date\"] = (row[0].split(\",\"))[1].split(\" \")[0]\n",
    "                row_values[\"hour\"] = int((row[0].split(\",\"))[1].split(\" \")[1].split(\":\")[0])\n",
    "                row_values[\"OT\"] = float((row[0].split(\",\"))[2])\n",
    "                id = int(row[0].split(\",\")[0])\n",
    "                data[id] = row_values\n",
    "            i += 1\n",
    "        return data\n",
    "\n",
    "data = get_data_from_csv('ETTh1_without_missing.csv')\n",
    "print(data[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-06-22 16:00:00\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Get the last value to predict the next 100 values\n",
    "last_entry = data[max(data.keys())]\n",
    "\n",
    "# Extract the date and hour from the last value\n",
    "last_date = datetime.strptime(last_entry['date'], \"%Y-%m-%d\")  # Convert the date string to a datetime object\n",
    "last_hour = last_entry['hour']\n",
    "\n",
    "next_100_dates = []  # List to store the next 100 dates\n",
    "\n",
    "# Generate the next 100 dates\n",
    "for _ in range(100):\n",
    "    last_hour += 1  # Increment the hour by 1\n",
    "    next_date = last_date + timedelta(hours=last_hour)  # Calculate the next date\n",
    "\n",
    "    # Format the next date to the desired string format\n",
    "    next_date_formatted = next_date.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    next_100_dates.append(next_date_formatted)  # Append the formatted next date to the list\n",
    "\n",
    "# The list next_100_dates now contains the next 100 dates\n",
    "print(next_100_dates[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "# extract data to learn on\n",
    "extracted_data = {cle: valeur for i, (cle, valeur) in enumerate(data.items()) if i >= len(data)-1000}\n",
    "print(len(extracted_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Nassim\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nassim\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set length : 722 samples\n",
      "Validation set length : 128 samples\n",
      "Test set length : 150 samples\n",
      "Total : 1000 samples\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Extract the dates from the data by converting strings to datetime objects\n",
    "dates = [datetime.strptime(value['date'], '%Y-%m-%d') for _, value in extracted_data.items()]\n",
    "\n",
    "# Extract the years, months, days, and hours from the datetime objects\n",
    "years = [date.year for date in dates]\n",
    "months = [date.month for date in dates]\n",
    "days = [date.day for date in dates]\n",
    "hours = [value['hour'] for _, value in extracted_data.items()]\n",
    "\n",
    "# Extract the targets (OT values) from the data\n",
    "targets = [value['OT'] for _, value in extracted_data.items()]\n",
    "\n",
    "# Stack the years, months, days, and hours into a numpy array\n",
    "input = np.column_stack((years, months, days, hours))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "# 80% of the data will be used for training and 20% for testing\n",
    "# Data is split randomly, with random_state=42 to ensure reproducibility\n",
    "X_train, X_test, y_train, y_test = train_test_split(input, targets, test_size=0.15, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(np.array(X_train), np.array(y_train), test_size=0.15, random_state=42)\n",
    "\n",
    "# Create TensorFlow datasets from arrays\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "\n",
    "batch_size_value = 32\n",
    "# Shuffle and batch the training data\n",
    "train_dataset = train_dataset.shuffle(buffer_size=len(X_train)).batch(batch_size_value)\n",
    "\n",
    "# Batch the validation and test data\n",
    "val_dataset = val_dataset.batch(batch_size_value)\n",
    "test_dataset = test_dataset.batch(batch_size_value)\n",
    "\n",
    "# Display set lengths\n",
    "print(\"Train set length :\", len(X_train), \"samples\")\n",
    "print(\"Validation set length :\", len(X_val), \"samples\")\n",
    "print(\"Test set length :\", len(X_test), \"samples\")\n",
    "print(\"Total :\", len(X_train)+len(X_test)+len(X_val), \"samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Nassim\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Nassim\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:From c:\\Users\\Nassim\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Nassim\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "23/23 [==============================] - 3s 20ms/step - loss: 156.1490 - mae: 9.5836 - val_loss: 24.8496 - val_mae: 4.4838\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.9101 - mae: 2.9291 - val_loss: 13.3419 - val_mae: 3.1771\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 7.5081 - mae: 2.1515 - val_loss: 6.9161 - val_mae: 1.8961\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 6.0383 - mae: 1.8654 - val_loss: 6.2525 - val_mae: 1.7637\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 6.5549 - mae: 1.9750 - val_loss: 6.5074 - val_mae: 1.8142\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 6.1483 - mae: 1.8943 - val_loss: 6.1444 - val_mae: 1.8572\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 5.7477 - mae: 1.8246 - val_loss: 6.0888 - val_mae: 1.8416\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 5.8965 - mae: 1.8777 - val_loss: 5.7710 - val_mae: 1.7304\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 5.6577 - mae: 1.8112 - val_loss: 6.7775 - val_mae: 1.8695\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 6.3779 - mae: 1.9122 - val_loss: 6.3236 - val_mae: 1.9139\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 6.1747 - mae: 1.9004 - val_loss: 8.6638 - val_mae: 2.2484\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 8.1429 - mae: 2.2793 - val_loss: 10.0697 - val_mae: 2.5086\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 6.0936 - mae: 1.8792 - val_loss: 5.9638 - val_mae: 1.8123\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 6.2637 - mae: 1.9436 - val_loss: 8.6124 - val_mae: 2.3985\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 7.2769 - mae: 2.1187 - val_loss: 7.9706 - val_mae: 2.2799\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 7.1722 - mae: 2.0972 - val_loss: 14.9972 - val_mae: 3.4148\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 9.3843 - mae: 2.4478 - val_loss: 5.9668 - val_mae: 1.8169\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 6.6541 - mae: 1.9854 - val_loss: 7.6410 - val_mae: 2.2182\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 8.0085 - mae: 2.2409 - val_loss: 11.3887 - val_mae: 2.7366\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 6.7900 - mae: 2.0320 - val_loss: 8.6361 - val_mae: 2.4057\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 7.8307 - mae: 2.1982 - val_loss: 6.9815 - val_mae: 2.0828\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 6.4205 - mae: 1.9425 - val_loss: 5.9092 - val_mae: 1.8055\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 7.2064 - mae: 2.1166 - val_loss: 12.0819 - val_mae: 2.9813\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 8.7367 - mae: 2.3448 - val_loss: 8.5517 - val_mae: 2.2215\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 7.1289 - mae: 2.0571 - val_loss: 5.7148 - val_mae: 1.7204\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 6.6803 - mae: 1.9839 - val_loss: 5.9219 - val_mae: 1.7150\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 6.4239 - mae: 1.9956 - val_loss: 5.7581 - val_mae: 1.6986\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 7.6214 - mae: 2.1405 - val_loss: 5.9975 - val_mae: 1.7263\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 7.7769 - mae: 2.1678 - val_loss: 6.5061 - val_mae: 1.9776\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 9.5788 - mae: 2.5050 - val_loss: 6.2228 - val_mae: 1.9035\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 7.4099 - mae: 2.1291 - val_loss: 6.7387 - val_mae: 1.8693\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 6.3881 - mae: 1.9542 - val_loss: 5.7528 - val_mae: 1.6987\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 6.3327 - mae: 1.9387 - val_loss: 6.0928 - val_mae: 1.8645\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 7.6677 - mae: 2.1464 - val_loss: 6.8823 - val_mae: 1.8987\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 6.7745 - mae: 2.0320 - val_loss: 5.6973 - val_mae: 1.7021\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 5.9200 - mae: 1.8574 - val_loss: 5.7897 - val_mae: 1.7013\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 7.1534 - mae: 2.0682 - val_loss: 6.4067 - val_mae: 1.8031\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 9.2165 - mae: 2.4541 - val_loss: 9.2260 - val_mae: 2.3536\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 8.5664 - mae: 2.3313 - val_loss: 6.0522 - val_mae: 1.8545\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 6.5652 - mae: 1.9744 - val_loss: 6.7391 - val_mae: 1.8717\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 6.6265 - mae: 1.9813 - val_loss: 6.9028 - val_mae: 1.9044\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 7.7642 - mae: 2.2363 - val_loss: 6.1727 - val_mae: 1.7584\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 6.9681 - mae: 2.0747 - val_loss: 8.1697 - val_mae: 2.1472\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 6.7147 - mae: 2.0189 - val_loss: 8.2059 - val_mae: 2.3328\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 8.9256 - mae: 2.3977 - val_loss: 7.6105 - val_mae: 2.2180\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 5.8636 - mae: 1.8576 - val_loss: 5.8305 - val_mae: 1.7853\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 6.9572 - mae: 2.0779 - val_loss: 10.5059 - val_mae: 2.7292\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 8.2658 - mae: 2.3113 - val_loss: 8.9712 - val_mae: 2.4696\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 8.6385 - mae: 2.3568 - val_loss: 7.7892 - val_mae: 2.0739\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 7.1870 - mae: 2.1130 - val_loss: 5.6942 - val_mae: 1.7248\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 6.0752 - mae: 1.7644\n",
      "Test loss : 6.075157642364502\n",
      "Test MAE : 1.7643616199493408\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Model creation\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(units=128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(units=64, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=32, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=16, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=1)  \n",
    "])\n",
    "\n",
    "# Compilation\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# Training\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=batch_size_value, validation_data=(X_val, np.array(y_val)))\n",
    "\n",
    "# Evaluation\n",
    "loss, mae = model.evaluate(X_test, np.array(y_test))\n",
    "print(\"Test loss :\", loss)\n",
    "print(\"Test MAE :\", mae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_next_date(date, hour):\n",
    "    \n",
    "    years = int(date.split(\"-\")[0])\n",
    "    months = int(date.split(\"-\")[1])\n",
    "    days = int(date.split(\"-\")[2])\n",
    "\n",
    "    return np.array([[years, months, days, hour]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 210ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "CSV file 'neural_network_prediction.csv' successfully created.\n"
     ]
    }
   ],
   "source": [
    "def predict_next_100_values(filename):\n",
    "    fields = [\"Id\", \"OT\"]\n",
    "\n",
    "    with open(filename, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(fields)\n",
    "        \n",
    "        for i in range(100):\n",
    "            csv_line = []\n",
    "            date = next_100_dates[i].split(\" \")[0]\n",
    "            hour = int(next_100_dates[i].split(\" \")[1].split(\":\")[0])\n",
    "            new_value_prediction = model.predict(preprocess_next_date(date, hour))\n",
    "            csv_line = [i, float(new_value_prediction)]\n",
    "            writer.writerow(csv_line)\n",
    "    print(f\"CSV file '{filename}' successfully created.\")\n",
    "\n",
    "filename = \"neural_network_prediction.csv\"\n",
    "predict_next_100_values(filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
