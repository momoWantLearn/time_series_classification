{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def partionnage(chemin):\n",
    "    row=[]\n",
    "    with open(chemin, 'r') as file:\n",
    "        i=0\n",
    "        reader = csv.reader(file, delimiter=';')\n",
    "        new_row={}\n",
    "        for row in reader:\n",
    "            if(i!=0):\n",
    "                temp_dic={}\n",
    "                temp_dic[\"date\"]=(row[0].split(\",\"))[1].split(\" \")[0]\n",
    "                temp_dic[\"heure\"]=int((row[0].split(\",\"))[1].split(\" \")[1].split(\":\")[0])\n",
    "                temp_dic[\"OT\"]=float((row[0].split(\",\"))[2])\n",
    "                new_row[int((row[0].split(\",\"))[0])]=temp_dic\n",
    "            i=i+1\n",
    "        return new_row\n",
    "    \n",
    "\n",
    "data=partionnage('ETTh1_without_missing.csv')\n",
    "data={cle: valeur for i, (cle, valeur) in enumerate(data.items()) if i > 7000}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 297.2719 - mae: 10.0409 - val_loss: 11.2595 - val_mae: 2.6478\n",
      "Epoch 2/10\n",
      "256/256 [==============================] - 0s 1ms/step - loss: 51.5249 - mae: 6.1762 - val_loss: 17.2051 - val_mae: 3.4944\n",
      "Epoch 3/10\n",
      "256/256 [==============================] - 0s 1ms/step - loss: 51.2714 - mae: 6.1758 - val_loss: 8.8298 - val_mae: 2.2460\n",
      "Epoch 4/10\n",
      "256/256 [==============================] - 0s 1ms/step - loss: 52.8325 - mae: 6.2190 - val_loss: 7.4406 - val_mae: 2.1171\n",
      "Epoch 5/10\n",
      "256/256 [==============================] - 0s 1ms/step - loss: 52.3995 - mae: 6.2065 - val_loss: 7.5470 - val_mae: 2.1079\n",
      "Epoch 6/10\n",
      "256/256 [==============================] - 0s 1ms/step - loss: 60.2496 - mae: 6.4947 - val_loss: 36.1200 - val_mae: 5.4075\n",
      "Epoch 7/10\n",
      "256/256 [==============================] - 0s 1ms/step - loss: 61.8995 - mae: 6.5732 - val_loss: 8.5261 - val_mae: 2.3486\n",
      "Epoch 8/10\n",
      "256/256 [==============================] - 0s 1ms/step - loss: 56.5470 - mae: 6.3790 - val_loss: 17.2034 - val_mae: 3.5280\n",
      "Epoch 9/10\n",
      "256/256 [==============================] - 0s 1ms/step - loss: 58.3047 - mae: 6.4265 - val_loss: 18.7194 - val_mae: 3.6285\n",
      "Epoch 10/10\n",
      "256/256 [==============================] - 0s 1ms/step - loss: 53.7197 - mae: 6.2640 - val_loss: 12.6205 - val_mae: 2.8296\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 18.9080 - mae: 3.9213\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 18.9080 - mae: 3.9213\n",
      "mae= 3.921283006668091\n",
      "4/4 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datetime import datetime\n",
    "from datetime import datetime\n",
    "\n",
    "#sans normalisation\n",
    "def predictionNeuronne(data):\n",
    "    \n",
    "    dates = [datetime.strptime(value['date'], '%Y-%m-%d') for key, value in data.items()]\n",
    "    years = [date.year for date in dates]\n",
    "    months = [date.month for date in dates]\n",
    "    days = [date.day for date in dates]\n",
    "    hours = [value['heure'] for key, value in data.items()]\n",
    "    ots = [value['OT'] for key, value in data.items()]\n",
    "    \n",
    "    features = np.column_stack((years, months, days, hours))  # Pas de normalisation\n",
    "    test_start_index = len(features) - 100\n",
    "    X_train = np.array(features[:test_start_index])\n",
    "    y_train = np.array(ots[:test_start_index])\n",
    "    X_test = np.array(features[test_start_index:])\n",
    "    y_test = np.array(ots[test_start_index:])\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "    \n",
    "    loss, mae = model.evaluate(X_test, y_test)\n",
    "    print(\"mae=\",model.evaluate(X_test, y_test)[1])\n",
    "    headers = [\"id\", \"Valeur attendu\", \"Valeur estime\"] \n",
    "    \n",
    "    predictions_scaled = model.predict(X_test)\n",
    "    # Création des résultats pour chaque exemple\n",
    "    resp = []\n",
    "    for i in range(len(X_test)):\n",
    "        id_val = 17219 + i\n",
    "        actual_value = y_test[i]\n",
    "        predicted_value = predictions_scaled[i]\n",
    "        resp.append([id_val, actual_value, predicted_value])\n",
    "\n",
    "    # Écriture des résultats dans un fichier CSV\n",
    "    filename = \"submissionReseauDeNeuronne.csv\"\n",
    "    with open(filename, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"id\", \"Valeur attendue\", \"Valeur estimée\"])\n",
    "        writer.writerows(resp)\n",
    "\n",
    "   \n",
    "    \n",
    "\n",
    "def pretraitementNewValue1(date,hour):\n",
    "    years = int(date.split(\"-\")[0])\n",
    "    months = int(date.split(\"-\")[1])\n",
    "    days = int(date.split(\"-\")[2])\n",
    "    return  np.column_stack((years, months, days, hour))\n",
    "\n",
    "\n",
    "def prediction(date,hour,data):\n",
    "    \n",
    "    features =pretraitementNewValue1(date,hour)\n",
    "    predicted_scale=predictionNeuronne(data)\n",
    "    return  predicted_scale.predict(features)\n",
    "   \n",
    "\n",
    "#sans normalisation\n",
    "\n",
    "predictionNeuronne(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 321.8839 - mae: 10.0575 - val_loss: 12.2986 - val_mae: 2.7271\n",
      "Epoch 2/10\n",
      "256/256 [==============================] - 0s 1ms/step - loss: 50.3323 - mae: 6.1291 - val_loss: 22.7042 - val_mae: 4.0554\n",
      "Epoch 3/10\n",
      "256/256 [==============================] - 0s 1ms/step - loss: 50.9638 - mae: 6.1456 - val_loss: 7.9700 - val_mae: 2.2248\n",
      "Epoch 4/10\n",
      "256/256 [==============================] - 0s 1ms/step - loss: 50.2992 - mae: 6.1349 - val_loss: 21.6654 - val_mae: 3.9543\n",
      "Epoch 5/10\n",
      "256/256 [==============================] - 0s 1ms/step - loss: 52.3452 - mae: 6.2277 - val_loss: 30.2651 - val_mae: 4.8602\n",
      "Epoch 6/10\n",
      "256/256 [==============================] - 0s 1ms/step - loss: 53.8857 - mae: 6.2967 - val_loss: 30.5751 - val_mae: 4.8924\n",
      "Epoch 7/10\n",
      "256/256 [==============================] - 0s 1ms/step - loss: 52.9408 - mae: 6.2184 - val_loss: 39.5753 - val_mae: 5.6982\n",
      "Epoch 8/10\n",
      "256/256 [==============================] - 0s 1ms/step - loss: 55.1148 - mae: 6.3175 - val_loss: 9.5068 - val_mae: 2.5226\n",
      "Epoch 9/10\n",
      "256/256 [==============================] - 0s 1ms/step - loss: 55.8584 - mae: 6.3149 - val_loss: 8.3925 - val_mae: 2.3224\n",
      "Epoch 10/10\n",
      "256/256 [==============================] - 0s 1ms/step - loss: 54.9254 - mae: 6.3286 - val_loss: 7.7772 - val_mae: 2.1982\n",
      "4/4 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "#en normalisant tout\n",
    "\n",
    "def pretraitementNewValue(date, hour, scaler_year, scaler_month, scaler_day, scaler_hour):\n",
    "    years = int(date.split(\"-\")[0])\n",
    "    months = int(date.split(\"-\")[1])\n",
    "    days = int(date.split(\"-\")[2])\n",
    "    hour_scaled = scaler_hour.transform([[hour]])\n",
    "    return np.column_stack((scaler_year.transform([[years]]),\n",
    "                            scaler_month.transform([[months]]),\n",
    "                            scaler_day.transform([[days]]),\n",
    "                            hour_scaled))\n",
    "def predictionNeuronne(data):\n",
    "    dates = [datetime.strptime(value['date'], '%Y-%m-%d') for key, value in data.items()]\n",
    "    years = [date.year for date in dates]\n",
    "    months = [date.month for date in dates]\n",
    "    days = [date.day for date in dates]\n",
    "    hours = [value['heure'] for key, value in data.items()]\n",
    "    ots = [value['OT'] for key, value in data.items()]\n",
    "    \n",
    "    scaler_year = StandardScaler()\n",
    "    scaler_month = StandardScaler()\n",
    "    scaler_day = StandardScaler()\n",
    "    scaler_hour = StandardScaler()\n",
    "    scaler_ot = StandardScaler()\n",
    "    \n",
    "    years_scaled = scaler_year.fit_transform(np.array(years).reshape(-1, 1))\n",
    "    months_scaled = scaler_month.fit_transform(np.array(months).reshape(-1, 1))\n",
    "    days_scaled = scaler_day.fit_transform(np.array(days).reshape(-1, 1))\n",
    "    hours_scaled = scaler_hour.fit_transform(np.array(hours).reshape(-1, 1))\n",
    "    ots_scaled = scaler_ot.fit_transform(np.array(ots).reshape(-1, 1))\n",
    "    \n",
    "    features = np.column_stack((years_scaled, months_scaled, days_scaled, hours_scaled))\n",
    "    \n",
    "    features = np.column_stack((years, months, days, hours))  # Pas de normalisation\n",
    "    test_start_index = len(features) - 100\n",
    "    X_train = np.array(features[:test_start_index])\n",
    "    y_train = np.array(ots[:test_start_index])\n",
    "    X_test = np.array(features[test_start_index:])\n",
    "    y_test = np.array(ots[test_start_index:])\n",
    "    \n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "    loss, mae = model.evaluate(X_test, y_test)\n",
    "    print(\"mae=\",model.evaluate(X_test, y_test)[1])\n",
    "    headers = [\"id\", \"Valeur attendu\", \"Valeur estime\"] \n",
    "    # Evaluation du modèle sur l'ensemble de test\n",
    "    predictions_scaled = model.predict(X_test)\n",
    "    \n",
    "    # Inverser la normalisation pour les prédictions de la variable OT\n",
    "\n",
    "    # Création des résultats pour chaque exemple\n",
    "    resp = []\n",
    "    for i in range(len(X_test)):\n",
    "        id_val = 17219 + i\n",
    "        actual_value = y_test[i] \n",
    "        predicted_value = predictions_scaled[i]\n",
    "        resp.append([id_val, actual_value, predicted_value])\n",
    "\n",
    "    # Écriture des résultats dans un fichier CSV\n",
    "    filename = \"submissionReseauDeNeuronneNonNormalisee.csv\"\n",
    "    with open(filename, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"id\", \"Valeur attendue\", \"Valeur estimée\"])\n",
    "        writer.writerows(resp)\n",
    "\n",
    "predictionNeuronne(data)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
