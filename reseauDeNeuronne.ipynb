{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def partionnage(chemin):\n",
    "    row=[]\n",
    "    with open(chemin, 'r') as file:\n",
    "        i=0\n",
    "        reader = csv.reader(file, delimiter=';')\n",
    "        new_row={}\n",
    "        for row in reader:\n",
    "            if(i!=0):\n",
    "                temp_dic={}\n",
    "                temp_dic[\"date\"]=(row[0].split(\",\"))[1].split(\" \")[0]\n",
    "                temp_dic[\"heure\"]=int((row[0].split(\",\"))[1].split(\" \")[1].split(\":\")[0])\n",
    "                temp_dic[\"OT\"]=float((row[0].split(\",\"))[2])\n",
    "                new_row[int((row[0].split(\",\"))[0])]=temp_dic\n",
    "            i=i+1\n",
    "        return new_row\n",
    "    \n",
    "\n",
    "data=partionnage('ETTh1_without_missing.csv')\n",
    "dataFinale=partionnage('ETTh1_without_missing.csv')\n",
    "data={cle: valeur for i, (cle, valeur) in enumerate(data.items()) if i < len(data) - 100}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jerem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\jerem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\jerem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From C:\\Users\\jerem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\jerem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "345/345 [==============================] - 1s 2ms/step - loss: 222.1316 - mae: 9.2916 - val_loss: 81.5384 - val_mae: 7.4717\n",
      "Epoch 2/10\n",
      "345/345 [==============================] - 1s 1ms/step - loss: 81.5436 - mae: 7.0815 - val_loss: 75.3019 - val_mae: 6.4280\n",
      "Epoch 3/10\n",
      "345/345 [==============================] - 0s 1ms/step - loss: 83.4672 - mae: 7.1578 - val_loss: 76.9635 - val_mae: 7.2534\n",
      "Epoch 4/10\n",
      "345/345 [==============================] - 1s 2ms/step - loss: 91.5530 - mae: 7.4910 - val_loss: 72.9210 - val_mae: 6.3542\n",
      "Epoch 5/10\n",
      "345/345 [==============================] - 1s 2ms/step - loss: 85.9942 - mae: 7.2547 - val_loss: 69.0610 - val_mae: 6.4953\n",
      "Epoch 6/10\n",
      "345/345 [==============================] - 1s 1ms/step - loss: 82.1285 - mae: 7.0603 - val_loss: 81.7628 - val_mae: 7.5472\n",
      "Epoch 7/10\n",
      "345/345 [==============================] - 1s 2ms/step - loss: 77.6293 - mae: 6.8756 - val_loss: 68.9260 - val_mae: 6.4381\n",
      "Epoch 8/10\n",
      "345/345 [==============================] - 1s 2ms/step - loss: 82.0285 - mae: 7.0622 - val_loss: 73.7653 - val_mae: 6.3512\n",
      "Epoch 9/10\n",
      "345/345 [==============================] - 1s 2ms/step - loss: 84.4599 - mae: 7.1697 - val_loss: 94.5849 - val_mae: 8.2644\n",
      "Epoch 10/10\n",
      "345/345 [==============================] - 0s 1ms/step - loss: 85.9813 - mae: 7.2455 - val_loss: 91.3741 - val_mae: 6.8993\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 92.3263 - mae: 7.0157\n",
      "108/108 [==============================] - 0s 1ms/step - loss: 92.3263 - mae: 7.0157\n",
      "mae= 7.015746593475342\n",
      "1/1 [==============================] - 0s 138ms/step\n",
      "[[7.9155684]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datetime import datetime\n",
    "from datetime import datetime\n",
    "\n",
    "#sans normalisation\n",
    "def predictionNeuronne(data):\n",
    "    dates = [datetime.strptime(value['date'], '%Y-%m-%d') for key, value in data.items()]\n",
    "    years = [date.year for date in dates]\n",
    "    months = [date.month for date in dates]\n",
    "    days = [date.day for date in dates]\n",
    "    hours = [value['heure'] for key, value in data.items()]\n",
    "    ots = [value['OT'] for key, value in data.items()]\n",
    "    \n",
    "    features = np.column_stack((years, months, days, hours))  # Pas de normalisation\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, ots, test_size=0.2, random_state=42)\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "    X_test = np.array(X_test)\n",
    "    y_test = np.array(y_test)\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "    \n",
    "    loss, mae = model.evaluate(X_test, y_test)\n",
    "    print(\"mae=\",model.evaluate(X_test, y_test)[1])\n",
    "    return model\n",
    "\n",
    "def pretraitementNewValue(date,hour):\n",
    "    years = int(date.split(\"-\")[0])\n",
    "    months = int(date.split(\"-\")[1])\n",
    "    days = int(date.split(\"-\")[2])\n",
    "    return  np.column_stack((years, months, days, hour))\n",
    "\n",
    "\n",
    "def prediction(date,hour,data):\n",
    "    \n",
    "    features =pretraitementNewValue(date,hour)\n",
    "    predicted_scale=predictionNeuronne(data).predict(features)\n",
    "    \n",
    "    return predicted_scale\n",
    "\n",
    "#sans normalisation\n",
    "print(prediction(\"2018-06-30\",4,data))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "345/345 [==============================] - 1s 2ms/step - loss: 0.3085 - mae: 0.4179 - val_loss: 0.1484 - val_mae: 0.2951\n",
      "Epoch 2/10\n",
      "345/345 [==============================] - 0s 1ms/step - loss: 0.1411 - mae: 0.2902 - val_loss: 0.1336 - val_mae: 0.2815\n",
      "Epoch 3/10\n",
      "345/345 [==============================] - 0s 1ms/step - loss: 0.1320 - mae: 0.2808 - val_loss: 0.1316 - val_mae: 0.2783\n",
      "Epoch 4/10\n",
      "345/345 [==============================] - 0s 1ms/step - loss: 0.1274 - mae: 0.2757 - val_loss: 0.1261 - val_mae: 0.2723\n",
      "Epoch 5/10\n",
      "345/345 [==============================] - 1s 1ms/step - loss: 0.1248 - mae: 0.2727 - val_loss: 0.1235 - val_mae: 0.2699\n",
      "Epoch 6/10\n",
      "345/345 [==============================] - 0s 1ms/step - loss: 0.1209 - mae: 0.2682 - val_loss: 0.1220 - val_mae: 0.2683\n",
      "Epoch 7/10\n",
      "345/345 [==============================] - 0s 1ms/step - loss: 0.1196 - mae: 0.2667 - val_loss: 0.1191 - val_mae: 0.2647\n",
      "Epoch 8/10\n",
      "345/345 [==============================] - 0s 1ms/step - loss: 0.1168 - mae: 0.2636 - val_loss: 0.1215 - val_mae: 0.2643\n",
      "Epoch 9/10\n",
      "345/345 [==============================] - 0s 1ms/step - loss: 0.1163 - mae: 0.2624 - val_loss: 0.1185 - val_mae: 0.2631\n",
      "Epoch 10/10\n",
      "345/345 [==============================] - 0s 1ms/step - loss: 0.1143 - mae: 0.2609 - val_loss: 0.1146 - val_mae: 0.2593\n",
      "108/108 [==============================] - 0s 913us/step - loss: 0.1101 - mae: 0.2564\n",
      "108/108 [==============================] - 0s 911us/step - loss: 0.1101 - mae: 0.2564\n",
      "mae= 0.25635236501693726\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "[[10.142363]]\n"
     ]
    }
   ],
   "source": [
    "#en normalisant tout\n",
    "def predictionNeuronne(data):\n",
    "    dates = [datetime.strptime(value['date'], '%Y-%m-%d') for key, value in data.items()]\n",
    "    years = [date.year for date in dates]\n",
    "    months = [date.month for date in dates]\n",
    "    days = [date.day for date in dates]\n",
    "    hours = [value['heure'] for key, value in data.items()]\n",
    "    ots = [value['OT'] for key, value in data.items()]\n",
    "    \n",
    "    scaler_year = StandardScaler()\n",
    "    scaler_month = StandardScaler()\n",
    "    scaler_day = StandardScaler()\n",
    "    scaler_hour = StandardScaler()\n",
    "    scaler_ot = StandardScaler()\n",
    "    \n",
    "    years_scaled = scaler_year.fit_transform(np.array(years).reshape(-1, 1))\n",
    "    months_scaled = scaler_month.fit_transform(np.array(months).reshape(-1, 1))\n",
    "    days_scaled = scaler_day.fit_transform(np.array(days).reshape(-1, 1))\n",
    "    hours_scaled = scaler_hour.fit_transform(np.array(hours).reshape(-1, 1))\n",
    "    ots_scaled = scaler_ot.fit_transform(np.array(ots).reshape(-1, 1))\n",
    "    \n",
    "    features = np.column_stack((years_scaled, months_scaled, days_scaled, hours_scaled))\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, ots_scaled, test_size=0.2, random_state=42)\n",
    "    \n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "    \n",
    "    loss, mae = model.evaluate(X_test, y_test)\n",
    "    print(\"mae=\",model.evaluate(X_test, y_test)[1])\n",
    "    return model, scaler_year, scaler_month, scaler_day, scaler_hour, scaler_ot\n",
    "\n",
    "def pretraitementNewValue(date, hour, scaler_year, scaler_month, scaler_day, scaler_hour):\n",
    "    years = int(date.split(\"-\")[0])\n",
    "    months = int(date.split(\"-\")[1])\n",
    "    days = int(date.split(\"-\")[2])\n",
    "    hour_scaled = scaler_hour.transform([[hour]])\n",
    "    return np.column_stack((scaler_year.transform([[years]]),\n",
    "                            scaler_month.transform([[months]]),\n",
    "                            scaler_day.transform([[days]]),\n",
    "                            hour_scaled))\n",
    "\n",
    "def prediction(date, hour, data):\n",
    "    model, scaler_year, scaler_month, scaler_day, scaler_hour, scaler_ot = predictionNeuronne(data)\n",
    "    features = pretraitementNewValue(date, hour, scaler_year, scaler_month, scaler_day, scaler_hour)\n",
    "    predicted_scale = model.predict(features)\n",
    "    # Retrouver les valeurs originales de OT\n",
    "    predicted_ot = scaler_ot.inverse_transform(predicted_scale)\n",
    "    return predicted_ot\n",
    "\n",
    "print(prediction(\"2018-06-13\", 4, data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
